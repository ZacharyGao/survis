
@article{gibbsPlantPhenotypingActive2018,
	title = {Plant {Phenotyping}: {An} {Active} {Vision} {Cell} for {Three}-{Dimensional} {Plant} {Shoot} {Reconstruction}},
	volume = {178},
	issn = {0032-0889},
	shorttitle = {Plant {Phenotyping}},
	url = {https://doi.org/10.1104/pp.18.00664},
	doi = {10.1104/pp.18.00664},
	abstract = {Three-dimensional (3D) computer-generated models of plants are urgently needed to support both phenotyping and simulation-based studies such as photosynthesis modeling. However, the construction of accurate 3D plant models is challenging, as plants are complex objects with an intricate leaf structure, often consisting of thin and highly reflective surfaces that vary in shape and size, forming dense, complex, crowded scenes. We address these issues within an image-based method by taking an active vision approach, one that investigates the scene to intelligently capture images, to image acquisition. Rather than use the same camera positions for all plants, our technique is to acquire the images needed to reconstruct the target plant, tuning camera placement to match the plant’s individual structure. Our method also combines volumetric- and surface-based reconstruction methods and determines the necessary images based on the analysis of voxel clusters. We describe a fully automatic plant modeling/phenotyping cell (or module) comprising a six-axis robot and a high-precision turntable. By using a standard color camera, we overcome the difficulties associated with laser-based plant reconstruction methods. The 3D models produced are compared with those obtained from fixed cameras and evaluated by comparison with data obtained by x-ray microcomputed tomography across different plant structures. Our results show that our method is successful in improving the accuracy and quality of data obtained from a variety of plant types.},
	number = {2},
	urldate = {2024-03-28},
	journal = {Plant Physiology},
	author = {Gibbs, Jonathon A. and Pound, Michael and French, Andrew P. and Wells, Darren M. and Murchie, Erik and Pridmore, Tony},
	month = oct,
	year = {2018},
	keywords = {3D},
	pages = {524--534},
	file = {Full Text PDF:D\:\\Files\\Zotero\\storage\\RYQLXSHV\\Gibbs et al. - 2018 - Plant Phenotyping An Active Vision Cell for Three.pdf:application/pdf;Snapshot:D\:\\Files\\Zotero\\storage\\LZGGNB8E\\6116587.html:text/html},
}

@article{hartleyDomainAdaptationSynthetic2021,
	title = {Domain {Adaptation} of {Synthetic} {Images} for {Wheat} {Head} {Detection}},
	volume = {10},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2223-7747},
	url = {https://www.mdpi.com/2223-7747/10/12/2633},
	doi = {10.3390/plants10122633},
	abstract = {Wheat head detection is a core computer vision problem related to plant phenotyping that in recent years has seen increased interest as large-scale datasets have been made available for use in research. In deep learning problems with limited training data, synthetic data have been shown to improve performance by increasing the number of training examples available but have had limited effectiveness due to domain shift. To overcome this, many adversarial approaches such as Generative Adversarial Networks (GANs) have been proposed as a solution by better aligning the distribution of synthetic data to that of real images through domain augmentation. In this paper, we examine the impacts of performing wheat head detection on the global wheat head challenge dataset using synthetic data to supplement the original dataset. Through our experimentation, we demonstrate the challenges of performing domain augmentation where the target domain is large and diverse. We then present a novel approach to improving scores through using heatmap regression as a support network, and clustering to combat high variation of the target domain.},
	language = {en},
	number = {12},
	urldate = {2024-03-28},
	journal = {Plants},
	author = {Hartley, Zane K. J. and French, Andrew P.},
	month = dec,
	year = {2021},
	note = {Number: 12
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {deep learning, computer vision, domain adaptation, plant phenotyping, CycleGAN},
	pages = {2633},
	file = {Full Text PDF:D\:\\Files\\Zotero\\storage\\92GG9NZA\\Hartley and French - 2021 - Domain Adaptation of Synthetic Images for Wheat He.pdf:application/pdf},
}

@article{atanboriConvolutionalNeuralNetBased2019,
	title = {Convolutional {Neural} {Net}-{Based} {Cassava} {Storage} {Root} {Counting} {Using} {Real} and {Synthetic} {Images}},
	volume = {10},
	issn = {1664-462X},
	url = {https://www.frontiersin.org/journals/plant-science/articles/10.3389/fpls.2019.01516/full},
	doi = {10.3389/fpls.2019.01516},
	abstract = {{\textless}p{\textgreater}Cassava roots are complex structures comprising several distinct types of root. The number and size of the storage roots are two potential phenotypic traits reflecting crop yield and quality. Counting and measuring the size of cassava storage roots are usually done manually, or semi-automatically by first segmenting cassava root images. However, occlusion of both storage and fibrous roots makes the process both time-consuming and error-prone. While Convolutional Neural Nets have shown performance above the state-of-the-art in many image processing and analysis tasks, there are currently a limited number of Convolutional Neural Net-based methods for counting plant features. This is due to the limited availability of data, annotated by expert plant biologists, which represents all possible measurement outcomes. Existing works in this area either learn a direct image-to-count regressor model by regressing to a count value, or perform a count after segmenting the image. We, however, address the problem using a direct image-to-count prediction model. This is made possible by generating synthetic images, using a conditional Generative Adversarial Network (GAN), to provide training data for missing classes. We automatically form cassava storage root masks for any missing classes using existing ground-truth masks, and input them as a condition to our GAN model to generate synthetic root images. We combine the resulting synthetic images with real images to learn a direct image-to-count prediction model capable of counting the number of storage roots in real cassava images taken from a low cost aeroponic growth system. These models are used to develop a system that counts cassava storage roots in real images. Our system first predicts age group ('young' and 'old' roots; pertinent to our image capture regime) in a given image, and then, based on this prediction, selects an appropriate model to predict the number of storage roots. We achieve 91\% accuracy on predicting ages of storage roots, and 86\% and 71\% overall percentage agreement on counting 'old' and 'young' storage roots respectively. Thus we are able to demonstrate that synthetically generated cassava root images can be used to supplement missing root classes, turning the counting problem into a direct image-to-count prediction task.{\textless}/p{\textgreater}},
	language = {English},
	urldate = {2024-03-28},
	journal = {Frontiers in Plant Science},
	author = {Atanbori, John and Montoya-P, Maria Elker and Selvaraj, Michael Gomez and French, Andrew P. and Pridmore, Tony P.},
	month = nov,
	year = {2019},
	note = {Publisher: Frontiers},
	keywords = {Cassava (Manhiot esculenta), Deep learning (DL) approaches, Machine leaming, Plant phenotyping tool, Software   Engineering, conditional Generative Adversarial Network (GAN)},
	file = {Full Text:D\:\\Files\\Zotero\\storage\\GTVI6GNK\\Atanbori et al. - 2019 - Convolutional Neural Net-Based Cassava Storage Roo.pdf:application/pdf},
}

@article{gibbsActiveVisionSurface2020,
	title = {Active {Vision} and {Surface} {Reconstruction} for {3D} {Plant} {Shoot} {Modelling}},
	volume = {17},
	issn = {1557-9964},
	url = {https://ieeexplore.ieee.org/document/8698802},
	doi = {10.1109/TCBB.2019.2896908},
	abstract = {Plant phenotyping is the quantitative description of a plant's physiological, biochemical, and anatomical status which can be used in trait selection and helps to provide mechanisms to link underlying genetics with yield. Here, an active vision- based pipeline is presented which aims to contribute to reducing the bottleneck associated with phenotyping of architectural traits. The pipeline provides a fully automated response to photometric data acquisition and the recovery of three-dimensional (3D) models of plants without the dependency of botanical expertise, whilst ensuring a non-intrusive and non-destructive approach. Access to complete and accurate 3D models of plants supports computation of a wide variety of structural measurements. An Active Vision Cell (AVC) consisting of a camera-mounted robot arm plus combined software interface and a novel surface reconstruction algorithm is proposed. This pipeline provides a robust, flexible, and accurate method for automating the 3D reconstruction of plants. The reconstruction algorithm can reduce noise and provides a promising and extendable framework for high throughput phenotyping, improving current state-of-the-art methods. Furthermore, the pipeline can be applied to any plant species or form due to the application of an active vision framework combined with the automatic selection of key parameters for surface reconstruction.},
	number = {6},
	urldate = {2024-03-28},
	journal = {IEEE/ACM Transactions on Computational Biology and Bioinformatics},
	author = {Gibbs, Jonathon A. and Pound, Michael P. and French, Andrew P. and Wells, Darren M. and Murchie, Erik H. and Pridmore, Tony P.},
	month = nov,
	year = {2020},
	note = {Conference Name: IEEE/ACM Transactions on Computational Biology and Bioinformatics},
	keywords = {Calibration, Solid modeling, Three-dimensional displays, Computational modeling, Robots, plant phenotyping, 3D reconstruction, active vision, calibration, Surface reconstruction, 3D},
	pages = {1907--1917},
	file = {IEEE Xplore Abstract Record:D\:\\Files\\Zotero\\storage\\BDKALVGQ\\8698802.html:text/html;IEEE Xplore Full Text PDF:D\:\\Files\\Zotero\\storage\\8MX29QSM\\Gibbs et al. - 2020 - Active Vision and Surface Reconstruction for 3D Pl.pdf:application/pdf},
}

@article{fioraniFutureScenariosPlant2013,
	title = {Future {Scenarios} for {Plant} {Phenotyping}},
	volume = {64},
	issn = {1543-5008, 1545-2123},
	url = {https://www.annualreviews.org/content/journals/10.1146/annurev-arplant-050312-120137},
	doi = {10.1146/annurev-arplant-050312-120137},
	abstract = {With increasing demand to support and accelerate progress in breeding for novel traits, the plant research community faces the need to accurately measure increasingly large numbers of plants and plant parameters. The goal is to provide quantitative analyses of plant structure and function relevant for traits that help plants better adapt to low-input agriculture and resource-limited environments. We provide an overview of the inherently multidisciplinary research in plant phenotyping, focusing on traits that will assist in selecting genotypes with increased resource use efficiency. We highlight opportunities and challenges for integrating noninvasive or minimally invasive technologies into screening protocols to characterize plant responses to environmental challenges for both controlled and field experimentation. Although technology evolves rapidly, parallel efforts are still required because large-scale phenotyping demands accurate reporting of at least a minimum set of information concerning experimental protocols, data management schemas, and integration with modeling. The journey toward systematic plant phenotyping has only just begun.},
	language = {en},
	number = {Volume 64, 2013},
	urldate = {2024-03-28},
	journal = {Annual Review of Plant Biology},
	author = {Fiorani, Fabio and Schurr, Ulrich},
	month = apr,
	year = {2013},
	note = {Publisher: Annual Reviews},
	keywords = {review},
	pages = {267--291},
	file = {Snapshot:D\:\\Files\\Zotero\\storage\\6R5L6JY5\\annurev-arplant-050312-120137.html:text/html},
}

@misc{zhangAddingConditionalControl2023,
	title = {Adding {Conditional} {Control} to {Text}-to-{Image} {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2302.05543},
	doi = {10.48550/arXiv.2302.05543},
	abstract = {We present ControlNet, a neural network architecture to add spatial conditioning controls to large, pretrained text-to-image diffusion models. ControlNet locks the production-ready large diffusion models, and reuses their deep and robust encoding layers pretrained with billions of images as a strong backbone to learn a diverse set of conditional controls. The neural architecture is connected with "zero convolutions" (zero-initialized convolution layers) that progressively grow the parameters from zero and ensure that no harmful noise could affect the finetuning. We test various conditioning controls, eg, edges, depth, segmentation, human pose, etc, with Stable Diffusion, using single or multiple conditions, with or without prompts. We show that the training of ControlNets is robust with small ({\textless}50k) and large ({\textgreater}1m) datasets. Extensive results show that ControlNet may facilitate wider applications to control image diffusion models.},
	urldate = {2024-03-28},
	publisher = {arXiv},
	author = {Zhang, Lvmin and Rao, Anyi and Agrawala, Maneesh},
	month = nov,
	year = {2023},
	note = {arXiv:2302.05543 [cs]
version: 3},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Human-Computer Interaction, Computer Science - Multimedia, ControlNet},
	file = {arXiv Fulltext PDF:D\:\\Files\\Zotero\\storage\\T7DCRJYY\\Zhang et al. - 2023 - Adding Conditional Control to Text-to-Image Diffus.pdf:application/pdf;arXiv.org Snapshot:D\:\\Files\\Zotero\\storage\\CHML7KSA\\2302.html:text/html},
}

@inproceedings{hoDenoisingDiffusionProbabilistic2020,
	title = {Denoising {Diffusion} {Probabilistic} {Models}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html},
	abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.},
	urldate = {2024-03-28},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
	year = {2020},
	pages = {6840--6851},
	file = {Full Text PDF:D\:\\Files\\Zotero\\storage\\VQP8A375\\Ho et al. - 2020 - Denoising Diffusion Probabilistic Models.pdf:application/pdf},
}

@article{hallHighthroughputPlantPhenotyping2022,
	title = {High-throughput plant phenotyping: a role for metabolomics?},
	volume = {27},
	issn = {1360-1385},
	shorttitle = {High-throughput plant phenotyping},
	url = {https://www.sciencedirect.com/science/article/pii/S1360138522000309},
	doi = {10.1016/j.tplants.2022.02.001},
	abstract = {High-throughput (HTP) plant phenotyping approaches are developing rapidly and are already helping to bridge the genotype–phenotype gap. However, technologies should be developed beyond current physico-spectral evaluations to extend our analytical capacities to the subcellular level. Metabolites define and determine many key physiological and agronomic features in plants and an ability to integrate a metabolomics approach within current HTP phenotyping platforms has huge potential for added value. While key challenges remain on several fronts, novel technological innovations are upcoming yet under-exploited in a phenotyping context. In this review, we present an overview of the state of the art and how current limitations might be overcome to enable full integration of metabolomics approaches into a generic phenotyping pipeline in the near future.},
	number = {6},
	urldate = {2024-03-28},
	journal = {Trends in Plant Science},
	author = {Hall, Robert D. and D’Auria, John C. and Silva Ferreira, Antonio C. and Gibon, Yves and Kruszka, Dariusz and Mishra, Puneet and van de Zedde, Rick},
	month = jun,
	year = {2022},
	keywords = {plant phenotyping, data integration, metabolomics, multimodal sensing, phenomics, plant phenotype},
	pages = {549--563},
	file = {Full Text:D\:\\Files\\Zotero\\storage\\8W5DINEZ\\Hall et al. - 2022 - High-throughput plant phenotyping a role for meta.pdf:application/pdf;ScienceDirect Snapshot:D\:\\Files\\Zotero\\storage\\6GEZ9T3H\\S1360138522000309.html:text/html},
}

@misc{chenControl3DControllableTextto3D2023,
	title = {{Control3D}: {Towards} {Controllable} {Text}-to-{3D} {Generation}},
	shorttitle = {{Control3D}},
	url = {http://arxiv.org/abs/2311.05461},
	doi = {10.48550/arXiv.2311.05461},
	abstract = {Recent remarkable advances in large-scale text-to-image diffusion models have inspired a significant breakthrough in text-to-3D generation, pursuing 3D content creation solely from a given text prompt. However, existing text-to-3D techniques lack a crucial ability in the creative process: interactively control and shape the synthetic 3D contents according to users' desired specifications (e.g., sketch). To alleviate this issue, we present the first attempt for text-to-3D generation conditioning on the additional hand-drawn sketch, namely Control3D, which enhances controllability for users. In particular, a 2D conditioned diffusion model (ControlNet) is remoulded to guide the learning of 3D scene parameterized as NeRF, encouraging each view of 3D scene aligned with the given text prompt and hand-drawn sketch. Moreover, we exploit a pre-trained differentiable photo-to-sketch model to directly estimate the sketch of the rendered image over synthetic 3D scene. Such estimated sketch along with each sampled view is further enforced to be geometrically consistent with the given sketch, pursuing better controllable text-to-3D generation. Through extensive experiments, we demonstrate that our proposal can generate accurate and faithful 3D scenes that align closely with the input text prompts and sketches.},
	urldate = {2024-03-28},
	publisher = {arXiv},
	author = {Chen, Yang and Pan, Yingwei and Li, Yehao and Yao, Ting and Mei, Tao},
	month = nov,
	year = {2023},
	note = {arXiv:2311.05461 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Multimedia, control3D},
	file = {arXiv Fulltext PDF:D\:\\Files\\Zotero\\storage\\8CUIRBFY\\Chen et al. - 2023 - Control3D Towards Controllable Text-to-3D Generat.pdf:application/pdf;arXiv.org Snapshot:D\:\\Files\\Zotero\\storage\\LI2XBWDI\\2311.html:text/html},
}

@article{luGenerativeAdversarialNetworks2022,
	title = {Generative adversarial networks ({GANs}) for image augmentation in agriculture: {A} systematic review},
	volume = {200},
	issn = {0168-1699},
	shorttitle = {Generative adversarial networks ({GANs}) for image augmentation in agriculture},
	url = {https://www.sciencedirect.com/science/article/pii/S0168169922005233},
	doi = {10.1016/j.compag.2022.107208},
	abstract = {In agricultural image analysis, optimal model performance is keenly pursued for better fulfilling visual recognition tasks (e.g., image classification, segmentation, object detection and localization), in the presence of challenges with biological variability and unstructured environments. Large-scale, balanced and ground-truthed image datasets are tremendously beneficial but most often difficult to obtain to fuel the development of highly performant models. As artificial intelligence through deep learning is impacting analysis and modeling of agricultural images, image augmentation plays a crucial role in boosting model performance while reducing manual efforts for image collection and labelling, by algorithmically creating and expanding datasets. Beyond traditional data augmentation techniques, generative adversarial network (GAN) invented in 2014 in the computer vision community, provides a suite of novel approaches that can learn good data representations and generate highly realistic samples. Since 2017, there has been a growth of research into GANs for image augmentation or synthesis in agriculture for improved model performance. This paper presents an overview of the evolution of GAN architectures followed by a first systematic review of various applications in agriculture and food systems (https://github.com/Derekabc/GANs-Agriculture), involving a diversity of visual recognition tasks for plant health conditions, weeds, fruits (preharvest), aquaculture, animal farming, plant phenotyping as well as postharvest detection of fruit defects. Challenges and opportunities of GANs are discussed for future research.},
	urldate = {2024-03-31},
	journal = {Computers and Electronics in Agriculture},
	author = {Lu, Yuzhen and Chen, Dong and Olaniyi, Ebenezer and Huang, Yanbo},
	month = sep,
	year = {2022},
	keywords = {Deep Learning, Agriculture, GANs, Computer Vision, GAN, Image Augmentation},
	pages = {107208},
}
